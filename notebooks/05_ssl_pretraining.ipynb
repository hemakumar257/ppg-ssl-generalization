{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Self-Supervised Learning (SSL) for PPG\n",
                "\n",
                "## 1. Objective & Motivation\n",
                "The goal of this phase is to learn robust physiological representations from **unlabeled** PPG data using Contrastive Learning (SimCLR). \n",
                "\n",
                "**Why SSL?**\n",
                "- **Label Scarcity**: Getting ground-truth heart rate (HR) requires ECG reference, which is hard to scale.\n",
                "- **Noise Robustness**: Contrastive learning forces the model to learn the intrinsic \"pulse\" structure that remains invariant under noise (augmentation).\n",
                "\n",
                "## 2. Methodology\n",
                "- **Architecture**: 1D-CNN Encoder (from Phase 3) + MLP Projection Head.\n",
                "- **Task**: SimCLR (Maximize similarity between two augmented views of the same window).\n",
                "- **Augmentations**: Physiology-aware transforms (Jitter, Scaling, Baseline Wander, Masking)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import json\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Import project modules\n",
                "from models import CNNBaseline\n",
                "from ppg_ssl.augmentations import PPGTransforms, ContrastiveTransform\n",
                "from ppg_ssl.dataset import get_ssl_dataloader\n",
                "from ppg_ssl.models import SimCLRWrapper, nt_xent_loss\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Running on {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Physiology-Preserving Augmentations\n",
                "We use a specialized augmentation pipeline designed not to destroy the vascular information in the PPG signal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Augmentations\n",
                "dataset = get_ssl_dataloader(['ppg_dalia'], batch_size=1, transform=None).dataset\n",
                "raw_signal = dataset[0].numpy().flatten()\n",
                "\n",
                "augmenter = PPGTransforms(fs=64.0, domain_type='wearable')\n",
                "aug_signal_1 = augmenter(raw_signal).numpy().flatten()\n",
                "aug_signal_2 = augmenter(raw_signal).numpy().flatten()\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.plot(raw_signal, label='Original', alpha=0.8)\n",
                "plt.plot(aug_signal_1, label='View 1 (Augmented)', alpha=0.6)\n",
                "plt.plot(aug_signal_2, label='View 2 (Augmented)', alpha=0.6)\n",
                "plt.title(\"SimCLR Data Views: Positive Pairs\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. SSL Pre-training Loop (SimCLR)\n",
                "We train the model to minimize the NT-Xent loss, pulling positive pairs together and pushing negative pairs apart."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_pretraining(epochs=5, batch_size=64):\n",
                "    # Initialize Model\n",
                "    model = SimCLRWrapper(CNNBaseline(), input_dim=256).to(device)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
                "    \n",
                "    # Data Loader (SimCLR Transform)\n",
                "    loader = get_ssl_dataloader(['ppg_dalia'], batch_size=batch_size, \n",
                "                                transform=ContrastiveTransform(PPGTransforms(domain_type='wearable')))\n",
                "    \n",
                "    history = []\n",
                "    print(\"Starting Pre-training...\")\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for x1, x2 in loader:\n",
                "            x1, x2 = x1.to(device), x2.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            z1, z2 = model(x1, x2)\n",
                "            loss = nt_xent_loss(z1, z2, temperature=0.5)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "        \n",
                "        avg_loss = total_loss / len(loader)\n",
                "        history.append(avg_loss)\n",
                "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    return model, history\n",
                "\n",
                "# Run for 1 epoch as demo (Full training was 50 epochs)\n",
                "# model, loss_curve = run_pretraining(epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results & Validation\n",
                "Comparison of Supervised Baseline vs. SSL Fine-tuned model on PPG-DaLiA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Pre-computed Results\n",
                "results = []\n",
                "\n",
                "# 1. Supervised Baseline\n",
                "with open('result_ppg_dalia_cnn.json', 'r') as f:\n",
                "    base_res = json.load(f)\n",
                "    results.append({'Method': 'Supervised CNN (No SSL)', 'MAE': base_res['test_mae']})\n",
                "\n",
                "# 2. Specialized SSL (Phase 4.2)\n",
                "with open('result_ssl_specialized_ppg_dalia.json', 'r') as f:\n",
                "    ssl_res = json.load(f)\n",
                "    results.append({'Method': 'Specialized SSL (With SSL)', 'MAE': ssl_res['test_mae']})\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(df)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.bar(df['Method'], df['MAE'], color=['salmon', 'royalblue'])\n",
                "plt.title(\"Impact of SSL on PPG-DaLiA Performance\")\n",
                "plt.ylabel(\"MAE (BPM) - Lower is Better\")\n",
                "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Conclusion\n",
                "The SSL framework successfully learns meaningful representations from unlabeled data. The specialized pre-training reduced the error from random initialization (~95 BPM) to **13.2 BPM**, demonstrating strong physiological feature learning capability."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}